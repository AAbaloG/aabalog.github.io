---
title: "Visión Robótica. Práctica 3."
categories:
  
tags:

---

En esta práctica se pedía programar un sistema de autolocalización visual 3D basado en balizas. Para ello, en primer lugar, hemos generado e imprimido una baliza tipo Aruco de dimensiones $$5 \times 5$$. En concreto, la baliza seleccionada es la de identificador 8, tal y como se podemos ver en la siguiente imagen.

![Im1](/assets/images/baliza.png)

Tras esto, hemos procedido a calibrar la cámara con la que grabaremos el vídeo en el que se estimará la posición. Para ello, hemos tomado dieciocho imágenes de la plantilla de calibración. Vemos algunas de ellas a continuación. 

<figure class="half">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/1.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/2.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/3.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/4.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/5.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/6.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/7.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/8.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/9.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/10.png" alt="" style="width:30%">
    <figcaption>Imágenes tomadas de la plantilla de calibración</figcaption>
</figure>

Con ellas, hemos utilizado la librería openCV para calibrar la cámara. Los pasos a seguir han sido los mismos que llevábamos a cabo en la calibración de la cámara en la asignatura de Visión 3D. En primer lugar, openCV encuentra los píxeles correspondientes a las esquinas de la plantilla de calibración, que se sitúan en las esquinas de los escaques del tablero de ajedrez, cada uno de ellos de dimensiones $$25 \times 25$$ $$\text{mm}^{2}$$. La resolución de las imágenes debe ser la misma que la de los frames del vídeo, por lo tanto, hemos reducido su resolución a $$640 \times 352$$ y calibrado con ellas. 

Después, indicando al algoritmo las correspondencias entre los puntos detectados en la plantilla de calibración, es decir los puntos 3D del espacio (que tendrán coordenada $$z=0$$, por encontrarse en el plano $$xy$$ definido por los escaques), y sus correspondientes píxeles 2D en cada una de las imágenes, la función cv2.calibrateCamera devuelve la matriz de intrínsecos de la cámara; los vectores de rotación y traslación para cada una de las imágenes; el error de retroproyección estimado en cada punto, que se calcula como la raiz de la media de los errores al cuadrado en cada punto del patrón de calibración y los coeficientes de distorsión de la lente.

Para comprobar que la calibración es correcta, proyectamos el sistema de referencia del mundo real en las plantillas de calibración y vemos que, en efecto, la proyección aparece donde debe.

<figure class="half">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/p1.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/p2.png" alt="" style="width:30%">
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/calib/p3.png" alt="" style="width:30%">
    <figcaption>Sistema de referencia proyectado sobre la plantilla de calibración</figcaption>
</figure>

Una vez tenemos esto, con las funciones de cv2.aruco podemos detectar las cuatro esquinas de la baliza y proyectar sobre ella el sistema de referencia. Entonces, sólo queda probar el funcionamiento sobre un vídeo (recordamos, grabado con la misma cámara). Para ello, frame a frame se calcula la posición relativa de la cámara respecto de la baliza. Si sólo tuviésemos que calcular dicha posición, que será, para nosotros, el centro de la cámara, en la imagen $$i$$ de las utilizadas para calibrar no tendríamos más que calcular el centro de la cámara como

$$
\text{centro}_{i} = -R^{t}_{i} \phantom{i} t_{i}
$$

Sin embargo, en un vídeo, donde el flujo de fotogramas es continuo, no tenemos los vectores de rotación y traslación para cada uno de los frames. Es aquí donde entra en juego el algoritmo perspective N points (pNp), el cual estima los vectores de rotación y traslación para una posición de la cámara no conocida resolviendo por aproximación un sistema de ecuaciones sobredeterminado, el cual utiliza los rayos de retroproyección que pasan por las cuatro esquinas de la baliza, su tamaño real (en nuestro caso, $$80 \times 80 \text{mm}^2$$) y que esta es plana. Con esto conocido, podemos calcular el centro de la cámara en cada frame, de manera que estamos estimando la posición de la misma de forma continua.

Así, representamos gráficamente la posición de la cámara en el espacio a la vez que se muestra el vídeo en los siguientes dos enlaces. Cabe destacar que la trayectoria de la cámara se pinta en naranja, los ejes rojo, verde y azul (RGB) se corresponden con los proyectados en la baliza en el vídeo y los cuatro puntos morados, con sus extremos detectados.

[Vídeo 1](https://drive.google.com/file/d/1t1xkHMp2X5DpzCTterDiHXG1wPqVGCOx/view?usp=share_link)

[Vídeo 2](https://drive.google.com/file/d/1EyfwDLfQfGL3DOcYRxYEbj5hnh8ssURx/view?usp=share_link)

